0608：
1、vi /etc/sysconfig/network-scripts/ifcfg-eth0，修改ip为192.168.98.60
    vi /etc/hosts，添加以下4行（作为主机名解析？）
        192.168.98.61 node1
        192.168.98.62 node2
        192.168.98.63 node3
        192.168.98.64 node4
    vi /etc/sysconfig/network，修改主机名为node0：NETWORKING=YES HOSTNAME=node0

0609：
1、yum install vim提示YumRepo Error: All mirror URLs are not using ftp, http[s] or file.centos6 yum失败
参考这里修改镜像源为阿里镜像源（https://blog.csdn.net/weixin_45621658/article/details/110734514）
2、chkconfig：检查，设置系统的各种服务（redhat/centos）
    chkconfig xxxd on：xxx服务每次跟随系统启动
    service xxxd start：xxx服务本次立即启动
    （禁止防火墙iptables、使能网络时间同步ntpd）
3、rm -f /etc/udev/rules.d/70-persistent-net.rules：防止克隆虚拟机时重新自动生成ethxx（克隆前不要重启，因为会重新生成）
4、vim /etc/selinux/config，SELINUX=disabled：禁止安全子系统selinux
    vim /etc/ssh/sshd_config，UseDNS no：禁止sshd使用UseDNS
5、init 0：关机；init 6：重启
6、跳过P8（NAT_桥接_hosts）~P41（网络知识补充）

0611：
1、init 0关机后，拍个快照，准备克隆（注意：此时不能启动，否则会生成新的70-persistent-net.rules文件）：
(1)workstation->虚拟机->快照->快照管理器->拍摄快照->名称“初始化安装”->拍摄快照
(2)workstation->虚拟机->快照->快照管理器->克隆->现有快照->
    创建连接克隆（要保证原来的node0能正常工作，才可以正常使用链接克隆）->
    虚拟机名称“node1”->位置选择在node0文件夹同级下创建node1
2、node1开机，配置网络环境（和node0相同，都开机时会有冲突）和改主机名：
    vi /etc/sysconfig/network-scripts/ifcfg-eth0，修改ip为192.168.98.61
    vi /etc/sysconfig/network，修改主机名为node0：NETWORKING=YES HOSTNAME=node1
    同理再克隆node2、node3、node4
3、体验DR网络模型：处理高并发
（1）名词解释：
1.1：LVS：Linux Virtual Server，属于网络模型第四层的“虚拟服务器”，用户只能看到这个服务器，看不到后面的真实服务器；
1.2：LB：Load Balancer，“负载调度器”，运行在LVS上
1.3：DR：Virtual Server via Direct Routing，虚拟服务器背后有多个与其ip相同的真实服务器，但mac不同；
    DR模型就是把mac修改为想要转发到的目的真实服务器的mac，并透传过去；
（2）node1作为LB：
2.1：ifconfig eth0:3 192.168.98.60/24：配置一个虚拟ip
2.2：echo 1 > /proc/sys/net/ipv4/ip_forward：使能ip_forward，表示不丢弃不属于自己的以太数据、而是转发
2.3：yum install ipvsadm -y：安装“虚拟服务器”，才能真正成为虚拟服务器
    ipvsadm -A -t 192.168.98.60:80 -s rr：添加用作“虚拟服务器”中“负载调度器”的ip，且指定其调度算法为“rr轮询”
    ipvsadm -ln：确认上一个命令添加ip成功
2.4：ipvsadm -a -t 192.168.98.60:80 -r 192.168.98.62 -g：添加转发规则，虚拟服务器ip->真实服务器ip的映射
    ipvsadm -a -t 192.168.98.60:80 -r 192.168.98.63 -g：添加转发规则，虚拟服务器ip->真实服务器ip的映射
    ipvsadm -ln：确认上两个命令添加规则成功
（3）node2、node3作为真实（http）服务器：
3.1：echo 1 > /proc/sys/net/ipv4/conf/eth0/arp_ignore
    echo 2 > /proc/sys/net/ipv4/conf/eth0/arp_announce
    echo 1 > /proc/sys/net/ipv4/conf/all/arp_ignore
    echo 2 > /proc/sys/net/ipv4/conf/all/arp_announce：防止真实服务器的mac被PC记住、而绕过虚拟服务器直接访问到
3.2：ifconfig lo:8 192.168.98.60 netmask 255.255.255.255：
    真实服务器要配置一个和虚拟服务器相同的ip，以“伪装虚拟服务器”来承担“高并发访问”；
    （命令执行完可能会断链1min？）
3.3：yum install httpd -y：安装http服务器
    vim /var/www/html/index.html（并随便写一些内容）：创建一个简单的、可供浏览器访问的http文件
    service httpd restart：启动httpd服务器
（4）用浏览器打开http://192.168.98.60，连续刷新：就可以看到两个真实服务器返回给浏览器的字符串数据
（5）在node2、node3上执行netstat -natp：可以看到最近接收过的浏览器请求；
    在node1上执行ipvsadm -lnc：可以看到转发过的请求；
（6）尝试几次后可能浏览器直接与node2或node3通讯了，
4、遗留问题：DR模型
（1）如果将一台RS的httpd服务器关闭，发现请求的时候，响应如果转发到down掉的RS将会出现响应比较慢的情况，有时候还会显示找不到网页。虽然最终或通过启动RS再次给与响应，但是用户提要非常不好。
（2）如果LB服务器down掉，及时RS全部正常运行，用户请求将得不到响应。

0612：
1、keepalived：集群管理中保证“高可用”的服务软件。自动检测服务器的状态（心跳机制），发现与宕机、故障时，自动剔除问题机器、调度正常机器等。
（1）原理：使用keepalived服务的“虚拟路由器”使用了VRRP（Virtual Router Redundancy Protocol，虚拟路由冗余协议），
        其中协议规定虚拟mac地址为00-00-5E-00-01-<VRID>；
        真实服务器宕机时，能被虚拟路由器检测到；
        单个虚拟路由器宕机时，能自动切换到其它虚拟路由器（主备模式：主机恢复后切换回主机；主从模式：主机恢复后不切换回主机？）
（2）node1作为“虚拟路由器”主机：vim /etc/keepalived/keepalived.conf
    virtual_ipaddress 192.168.98.60/24 dev eth0 label eth0:8
    virtual_server 192.168.98.60 80
    lb_kind DR
    real_server 192.168.98.62 80
    SSL_GET改为HTTP_GET
    digest xxx改为status_code 200
    real_server 192.168.98.63 80
    persistence_timeout 0
（3）node1启用keepalived服务：
    ipvsadm -C：清除之前的配置
    service keepalived restart
（4）用浏览器打开http://192.168.98.60，并不断刷新：能看到页面信息在node2、node3之间切换
（5）node4作为“虚拟路由器”备机：
    在node1下执行scp /etc/keepalived/keepalived.conf node4:/etc/keepalived/keepalived.conf后，
    打开该文件：vim /etc/keepalived/keepalived.conf
    state BACKUP
    priority 50：修改node4优先级比MASTER的node1低（数值越大优先级越高）
（5）node4启用keepalived服务：
    ipvsadm -C：清除（可能存在的）之前的配置
    service keepalived restart
（6）测试1：node2宕机后，node1和node4作为虚拟服务器，都能检测到：
    在node2上执行service httpd stop：关闭http服务器
    在node1上执行ipvsadm -ln：查看检测到的正常http服务器：已经没有node2了；
    在node4上执行ipvsadm -ln：查看检测到的正常http服务器：已经没有node2了；
    在node2上执行service httpd start：重新启动http服务器
    在node1上执行ipvsadm -ln：查看检测到的正常http服务器：正常出现node2；
    在node4上执行ipvsadm -ln：查看检测到的正常http服务器：正常出现node2；
    说明：主、备虚拟服务器都能同步检测到真实服务器的宕机、恢复；
（7）测试2：node1宕机后，node4接替成为在用的“虚拟路由器”：（发生“虚拟路由器之间的虚拟ip漂移”）
    在node1上执行ifconfig eth0 down：“node1宕机”的定义：“虚拟网卡对应的真实网卡已断链”
    在node4上执行ifconfig：查看到keepalived服务创建了和原来node1相同的虚拟ip
    用浏览器打开http://192.168.98.60，并不断刷新：能看到页面信息在node2、node3之间切换
    在node4上执行ipvsadm -ln：查看到node4分别与node2、node3成功通讯的次数
    在node1上执行ifconfig eth0 up：node1从宕机中恢复
    在node4上执行ifconfig：查看到keepalived服务删除了虚拟ip（因为node1已恢复）
    用浏览器打开http://192.168.98.60，并不断刷新：能看到页面信息在node2、node3之间切换
    在node1上执行ipvsadm -ln：查看到node1分别与node2、node3成功通讯的次数
    说明：主、备虚拟服务器在keepalived服务下能实现自动切换
2、预览部分视频：
    Redis、Flink：好像都没有实际编程的内容，都是搭建环境、原理讲解、命令行操作；
    Hadoop、Storm：有实例编程，特别是Hadoop，至少有两个项目（天气、pagerank）的实操讲解；

0613：
1、P46：一般在七层做高并发，nginx做负载均衡、zookeeper做集群管理；keepalived是在四层
2、nginx是一个高性能（对比apache httpd）的http（轻量级web）和反向代理服务器；官方测试能够给支撑5w并发，并且cpu、内存等资源消耗却非常低，运行非常稳定；
    tengine是nginx的封装、高性能、淘宝开源版，支持10w并发量；
    学习还是nginx为主；
3、创建nginx1环境：
（1）从node0的“初始化安装”克隆新的虚拟机，名为nginx1：
    vim /etc/sysconfig/network：修改主机名为nginx1
    vim /etc/sysconfig/network-scripts/ifcfg-eth0：修改eth0 ip为192.168.98.65
    reboot
（2）安装、配置nginx：
    2.1：预先安装nginx依赖的库：
        yum install gcc gcc-c++ pcre  pcre-devel openssl openssl-devel zlib  zlib-devel -y
    2.2：从nginx官网下载nginx-1.16.1.tar.gz，在虚拟机nginx1上创建目录/opt/apps并上传到该路径，解压、编译、安装：
        mkdir -p /opt/apps
        cd /opt/apps
        cd nginx-1.16.1
        ./configure --prefix=/opt/nginx --with-http_ssl_module --with-http_gzip_static_module --error-log-path=/var/log/nginx/nginx.log --pid-path=/var/log/nginx/pid：简单配置安装项，只指定少数的几个参数
        make && make install
    2.3：使能nginx命令可用：（可通过添加环境变量的方式，但这里使用另一种）
        vim /etc/init.d/nginx：新建这么一个nginx脚本，粘贴内容挺多的，保存在windows下的1_3_nginx.txt文件
            支持service nginx status/start/stop/restart/reload命令
        chmod +x /etc/init.d/nginx：添加可执行权限（因为是脚本，对应“service nginx”命令）
        chkconfig --add nginx：添加为系统服务
        chkconfig --list nginx：查看是否添加成功
        chkconfig nginx on：设置开机启动
        reboot
        service nginx status：查看nginx是否正常启动
        ps aux | grep nginx：查看nginx是否正常启动
        浏览器打开http://192.168.98.65/：显示“Welcome to nginx”即表示正常

0614：
1、P48：“nginx用作http服务器的conf文件配置”之前的概览：
    id nobody：查看用户nobody是否存在（nginx默认用户）
    cat /proc/sys/fs/file-max：查看硬件最大支持打开文件的句柄数（1G内存对应97318）
    ulimit -a：查看当前对shell程序的资源限制
    ulimit -SHn 65535：设置软件、硬件限制的最大支持打开文件的数量为65535
    cat /opt/nginx/conf/nginx.conf：看到“sendfile on”表示使用高性能IO操作，
        使用DMA把数据从kernel_in_buffer[]直接传输到kernel_out_buffer[]，
        不经过user_buffer[]，从而减少kernel mode和user mode之间的切换
2、“nginx用作http服务器的conf文件配置”：
（1）要求：单个nginx服务器虚拟出两个主机（两个域名主机；也可以尝试端口主机，但很少用）：
    1.1：http://www.sxthenhao.com，连接到/mnt目录，支持下载里面的文件；
    1.2：http://www.123.com，能访问到简单的html页面（nginx欢迎页）
（2）配置步骤（注意文件内容都是用空格分隔，而不是逗号）：
    vim /opt/nginx/conf/nginx.conf：打开配置文件，里面的参数是nginx作为http服务器的一些配置
    使能gzip on：使能压缩传输
    keepalived_timeout 0：调试时先把该值设置为0
    修改一个（域名）主机http://www.sxthenhao.com：
        server {
            listen       80;
            server_name  www.sxthenhao.com;
            location / {
                root   /mnt;    //访问时挂载的内容为/mnt目录
                autoindex on;   //支持下载
            }
        }
    新增一个（域名）主机http://www.123.com：
        server {
            listen       80;
            server_name  www.123.com;
            location / {
                root   html;
                index index.html index_htm;
            }
        }
    service nginx restart
（3）访问多个虚拟主机：
    cp -rf /opt/apps/* /mnt：搞点东西到/mnt路径
    修改本地host文件（C:\Windows\System32\drivers\etc\hosts），添加域名：
        192.168.98.65 nginx1 www.123.com www.sxthenhao.com
        192.168.98.61 node1：这4行是给后面的实验用的
        192.168.98.62 node2
        192.168.98.63 node3
        192.168.98.64 node4
    浏览器访问http://www.sxthenhao.com：可以看到/mnt下的文件；
    浏览器访问http://www.123.com：可以看到nginx欢迎页html；
3、P49：“nginx用作log服务器的conf文件配置”之前的概览：
    tail -f /opt/nginx/logs/access.log：循环输出nginx默认log文件的尾部内容，实时更新
    浏览器访问http://www.sxthenhao.com：可以看到log尾部实时追加一些信息；
    浏览器访问http://www.123.com：可以看到log尾部实时追加一些信息；
    说明：nginx可提供log信息，因此可以作为log服务器
4、“nginx用作log服务器的conf文件配置”：
（1）要求：
（2）配置步骤（注意文件内容都是用空格分隔，而不是逗号）：
    vim /opt/nginx/conf/nginx.conf：打开配置文件，在http{}中进行配置
    log_format myfmt '$remote_addr - $remote_user [$time_local] "$request" '
        指定自定义的log格式
    在www.sxthenhao.com的server{}下添加access_log logs/myfmt.log myfmt：
        指定该server的log使用自定义的log格式、和保存的路径、文件名
    service nginx reload：重新加载nginx.conf文件
（3）访问多个log文件：
    tail -f /opt/nginx/logs/myfmt.log：循环输出nginx自定义log文件的尾部内容，实时更新
    tail -f /opt/nginx/logs/access.log：循环输出nginx默认log文件的尾部内容，实时更新
    浏览器访问http://www.sxthenhao.com：可以看到自定义log尾部实时追加一些信息，短了很多，说明使用了自定义的格式
    浏览器访问http://www.123.com：可以看到自定义log尾部并没有实时追加信息，而默认log尾部有，说明该server的log还在原来的access.log里；
（4）全局log配置：
    vim /opt/nginx/conf/nginx.conf
    在第一个server{}上面一行添加access_log logs/myfmt.log myfmt：即可全局生效（已测试）
5、（重要，但暂时不做实验，仅理解）nginx的server的location配置：
    用于指定浏览器访问server时，url的uri部分指向的内容；
    location对uri有好几种匹配模式（我自己说的）：匹配所有、精确匹配、模式匹配（正则表达式）
    ...
6、正向代理与反向代理：
（1）正向代理：proxy和client同属一个lan，对server透明，即proxy“保护”着client：
    1.1：防火墙功能
    1.2：缓冲功能
（2）反向代理：proxy和server同属一个lan，对client透明，即proxy“保护”着server：
    2.1：防火墙功能
    2.2：负载均衡功能
7、nginx的（软件）负载均衡：反向代理（对比keepalived在四层做负载均衡，nginx是在七层做的）
（1）方法1：
    vim /opt/nginx/conf/nginx.conf
    在第一个server之前添加upstream域名解析列表：表示nginx为这些真实服务器做反向代理
        upstream yyy{
            server 192.168.98.62;
            server 192.168.98.63;
        }
    在需要做负载均衡的server里添加相关location：
        location /toms {
            proxy_pass http://yyy/;
        }
    service nginx reload
    浏览器访问http://www.sxthenhao.com/toms：不断刷新，可以看到分别访问到node2、node3，说明实现了负载均衡
（2）方法2：
    vim /etc/hosts
    添加域名解析“192.168.98.62 xxx”
    添加域名解析“192.168.98.63 xxx”
    service nginx reload
    浏览器访问http://www.sxthenhao.com/cats：不断刷新，可以看到分别访问到node2、node3，说明实现了负载均衡
（3）方法1/2的对比：
    3.1：方法1直接在nginx.conf文件中添加server的ip列表，需要用“service nginx reload”命令重新加载才能生效；
        所以适合小企业的配置，不适合大企业；
    3.2：方法2通过改hosts文件的域名解析来更新server的ip列表，虽然上例有“service nginx reload”命令，
        但在大企业中有外部的DNS服务器，所以并不需要实际对nginx服务器reload，因此该方法适合大企业；

0616：
1、P52：session一致性：给实现反向代理服务器的nginx服务器，外加“内存数据库”功能（目前用memcached实现，以后用redis）
2、环境准备：
（1）node2、node3：
    1.1：service httpd start：使能httpd服务器；
    1.2：安装tomcat服务：
        （tomcat是基于java的Web应用服务器，需要通过端口号8080，才能访问到；否则是普通的http服务器）
        上传apache-tomcat-7.0.69.tar.gz到/opt/apps
            tar -xzvf apache-tomcat-7.0.69.tar.gz
            cd apache-tomcat-7.0.69/webapps/ROOT/
            vim index.jsp：修改欢迎页（改为简单的字符串内容，vim下用dG删除到文件末尾）
                from 192.168.98.62 <br/>：node3要注意填ip为63
                session=<%=session.getId()%>
            cd /opt/apps/apache-tomcat-7.0.69/bin
            ./startup.sh：这样才算启动tomcat服务
            ps aux | grep tomcat：确认tomcat已正常启动
    1.3：安装jdk：
        同样上传jdk-7u80-linux-x64.rpm到/opt/apps
        cd /opt/apps
        rpm -ivh jdk-7u80-linux-x64.rpm：默认安装为/usr/java/jdk1.7.0_80
        vim /etc/profile：在文件最后添加环境变量
            export JAVA_HOME=/usr/java/jdk1.7.0_80
            export PATH=$PATH:$JAVA_HOME/bin
        source /etc/profile：使新的环境变量生效
        jps：查看环境变量是否生效（正常输出类似“1722 Jps”）
    1.4：浏览器访问http://192.168.98.62:8080：看到node2的session（刷新不会变）
    1.5：浏览器访问http://192.168.98.63:8080：看到node3的session（刷新也不会变），和node2不一样，说明已经准备好“session不一致”的环境
（2）nginx1：
    2.1：yum install libevent -y：安装libevent（是一个“异步事件通知库”，用于“事件驱动的网络服务器”）
    2.2：yum install memcached -y：安装memcached（是一种“分布式的、基于内存的”key-value存储系统；以后会学redis代替这个）
    2.3：memcached -d -m 128m -p 11211 -l 192.168.98.65 -u root -P /tmp/：
        后台启动memcached：缓存128M，端口号11211，存储的文件路径在/tmp/
    2.4：ps aux | grep memcached：确认memcached已正常启动
（3）node2、node3：配置把session保存到nginx1服务器（以保持session一致性）
    3.1：/opt/apps/apache-tomcat-7.0.69/bin/shutdown.sh：先关闭tomcat服务
    3.2：vim /opt/apps/apache-tomcat-7.0.69/conf/context.xml：配置tomcat使用nginx1上的memcached系统
        在</Context>的上面添加内容：（具体含义这里不分析；以后更多地使用redis代替memcached）
        <Manager className="de.javakaffee.web.msm.MemcachedBackupSessionManager" 
            memcachedNodes="n1:192.168.98.65:11211" 
            sticky="false" 
            lockingMode="auto"
            sessionBackupAsync="false"
            requestUriIgnorePattern=".*\.(ico|png|gif|jpg|css|js)$"
            sessionBackupTimeout="1000" transcoderFactoryClass="de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory" 
        />
    3.3：cd /opt/apps/apache-tomcat-7.0.69/lib：
        上传9个相关的jar包到tomcat的/lib目录（才能实现“session一致性”功能）：
        asm-3.2.jar
        kryo-1.04.jar
        kryo-serializers-0.11.jar
        memcached-session-manager-1.7.0.jar
        memcached-session-manager-tc7-1.8.1.jar
        minlog-1.2.jar
        msm-kryo-serializer-1.7.0.jar
        reflectasm-1.01.jar
        spymemcached-2.7.3.jar
    3.4：/opt/apps/apache-tomcat-7.0.69/bin/startup.sh：重新启动tomcat服务
（4）nginx1：
    4.1：vim /opt/nginx/conf/nginx.conf：修改指定反向代理的真实服务器端口号为8080，表示对node2、node3的tomcat服务使用memcached
        upstream yyy{
            server 192.168.98.62:8080;
            server 192.168.98.63:8080;
        }
    4.2：service nginx restart
    4.3：浏览器打开http://www.sxthenhao.com/toms，不断刷新，可以看到：
        通过nginx反向代理可以轮流访问到192.168.98.62:8080和192.168.98.63:8080，
        更重要的是能同时保证了“session的一致性”
（5）待解决问题：浏览器刷新http://www.sxthenhao.com/toms是看到session还是会变，还没实现一致性？

0617：
1、vscode中git push报错如下时：
    fatal: unable to access 'https://github.com/MapleLeafFall/BigData.git/': OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to github.com:443
    解决办法：先git rebase一下，再git push即可。
        CWQ@CWQ-PC /cygdrive/e/2_Study/1_2_尚学堂笔记
        $ git rebase
        First, rewinding head to replay your work on top of it...
        Applying: P52:缓存一致性
        
        CWQ@CWQ-PC /cygdrive/e/2_Study/1_2_尚学堂笔记
        $ git push
        fatal: HttpRequestException encountered.
        发送请求时出错。
        Counting objects: 3, done.
        Delta compression using up to 4 threads.
        Compressing objects: 100% (3/3), done.
        Writing objects: 100% (3/3), 2.34 KiB | 0 bytes/s, done.
        Total 3 (delta 2), reused 0 (delta 0)
        remote: Resolving deltas: 100% (2/2), completed with 2 local objects.
        To https://github.com/MapleLeafFall/BigData.git
        7d907e1..a565a67  main -> main
2、问题未解决：即使在http://192.168.98.62:8080/和http://192.168.98.63:8080/下刷新，sessionId都会变（理论上只要浏览器不关闭就不会变才对）：
    原因未知；可能只在jar包或其它功能在安装的第一次有效？

0618：
1、P53：动静分离：（不做实验）
思路：新增一个nginx2服务器，上面放有静态页面资源，如image/css等，然后在nginx1也新增对它的反向代理。
    使得nginx2和node2、node3同级；
    浏览器向nginx1请求静态资源时（如：http://www.sxthenhao.com/image），会从nginx2获得；
    浏览器向nginx1请求动态资源时（如：http://www.sxthenhao.com/toms），会从node2、node3获得；
2、P55：高并发复习，理解以下名词：
tomcat（真实服务器的网页服务）
nginx、反向代理（用于高并发）、负载均衡、master进程、worker进程、（重要文件的路径）/usr/local/nginx/{conf,html,sbin,log}；
nginx的一些参数：sendfile on、upstream server、location/{...}等；
keepalived（可用于nginx服务器的高可用）、VRRP、VIP、VRID、IP漂移；
3、P56~Pxx：hadoop基础
（1）内容安排：6天课程=2天分布式存储+2天MapReduce+2天案例
（2）P56：1T文件问题（全排序）：
    1T文件分布在10000台内存为100M的服务器上，每台服务器上都是有序的，如何合并为一个有序的大文件？
    用归并排序；

0620：
1、P57：1T问题（分布式处理全排序问题）：
（1）并行，加快速度（2000s->1s）：把1T文件分为10000x100MB个小文件，保存到10000个服务器上；
（2）分布式运行（2000s->1s）：在10000个服务器上分别做排序；
（3）计算向数据移动（10000s->0s）：10000个服务器上做完排序后不再合并到大服务器上，而是留在本地（因为1T文件/千兆以太网=10000s）；
2、P58：hadoop介绍：
（1）hive：是基于hadoop的一个数据仓库工具（3天课程）；
    提供简单的sql查询功能，可以将sql语句转换为mapreduce任务作业；
    只会sql语句时，就用hive；
（2）spark：不需要读写hdfs；
（3）pig：高级过程语言，允许对分布式数据集进行类似sql的查询，可以简化hadoop的使用；
（4）zookeeper：分布式应用程序协调服务，包括配置维护、域名服务、分布式同步等；
（5）hbase：分布式存储系统，在hadoop上提供类似google bigtable的能力；
3、P59：hdfs架构与namenode
（1）hdfs client请求存储128M数据->namenode（副本数：默认=3）->记录该元数据自信息，并分别存到3个服务器
（2）namenode基本功能：
    2.1：心跳检测：服务器每3s发送一次心跳，失联后10min才由namenode恢复到新的服务器；
        因为namenode要管理集群的上万个节点、集群中存在网络延迟、失联恢复要占用带宽，所以不能实时处理心跳失联故障；
    2.2：负载均衡：
    2.3：复制（replication）：
（3）namenode特点：
    3.1：支持高并发访问：比如服务器集群datanode的硬件成本是1w台x5w块=5e，所以不是个人私用的，而是会有很多用户同时访问，所以要支持高并发；
    3.2：本地持久化文件：
        - 编辑日志（edits log，“追加文件”）：集群是有限的，但日志是无限的（会用到secondary namenode定期把log写到fsimage作为一部分）；
        - 命名空间镜像文件（fsimage，“内存快照”）：每个fsimage文件都是系统元数据的一个完整的持久化检查点（checkpoint，后缀表示镜像中的最后一个事务）；
            只有对磁盘的更新操作会更新这个数据，否则只更新log；
            又因为fsimage一般为GB量级，所以对其的“写”操作只更新log，不更新fsimage；
        - 基于上述两个特性的“重建元数据状态”：每次启动namenode时，将fsimage加载到内存，同时执行edits log对应于该fsimage之后的所有操作；
            就可以重建元数据的状态；
（4）secondary namenode基本功能：
    4.1：为namenode内存中的fsimage生成checkpoint：避免namenode重启后对fsimage花费大量时间进行edits.log的重演；
（5）secondary namenode特点：
    5.1：部分备份（namenode的fsimage）：每1min拷贝namenode的fsimage+sedits.log（用http get获取“事务数量”），每计时1h、
        或从edits.log发现edit次数达到1000000次就触发secondary namenode的“合并edits.log到fsimage，然后写入磁盘、回传给namenode更新”操作；
        （即checkpoint合并）
    5.2：secondary namenode要和namenode拥有相同的内存，甚至对于大的集群，secondary namenode运行于一台专用的物理主机；
4、P61：hdfs的datanode与副本存放策略、hadoop权限：
（1）datanode特点（存储block文件）：
    1.1：HDFS块数据存储于blk_前缀的文件中，包含了被存储文件原始字节数据的一部分；
    1.2：每个block文件都有一个同名的.meta文件关联：该文件包含了一个版本和类型信息的头部，后接该block中每个部分的校验和crc32；
        .meta文件很小，即使磁盘损坏，它的被破坏的几率非常小；
    1.3：单一block文件大小一致，文件与文件可以不一致：示例命令（调整或设置block大小为1MB、副本数为3）：
        hdfs dfs -D dfs.blocksize=1048576 -D dfs.replication=3 -put hello.txt /
    1.4：副本放置的默认策略（从源代码分析）：
        - 第一个副本：由集群外提交时：随机挑选一台磁盘不太慢、CPU不太忙的节点；
            由集群内提交时：直接放置在上传文件的该datanode；
        - 第二个副本：放置在与第一个副本不同机架的节点上（避免第一个机架断电故障的影响）；
        - 第三个副本：与第二个副本相同机架的不同节点（就近原则？）；
        - 更多副本：随机节点；
5、hadoop伪分布式搭建：
（1）即在同一个服务器（比如node1）上同时搭建：namenode、secondary namenode、datanode1、datanode2

0623：
1、hadoop伪分布式搭建：
（1）开发环境搭建：
    java ide：安装intellij idea version2018.3.6（视频用的是2018.3.2）
（2）hadoop环境（部署到node1）：
    2.1：复制hadoop-2.6.5.tar.gz到/root，并解压：tar -zxf hadoop-2.6.5.tar.gz -C /opt
    2.2：添加hadoop相关环境变量：vim /etc/profile
        export HADOOP_HOME=/opt/hadoop-2.6.5
        export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
    2.3：更新环境变量：source /etc/profile
（3）jdk环境（部署到node1）：
    3.1：复制jdk-8u221-linux-x64.rpm到/root，并安装：rpm -ivh jdk-8u221-linux-x64.rpm
    3.2：添加jdk相关环境变量：vim /etc/profile
        export JAVA_HOME=/usr/java/default
        export PATH=$PATH:$JAVA_HOME/bin
    3.3：更新环境变量：source /etc/profile
（4）配置免秘钥：
    ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa：
        生成ssh秘钥，同时用-P指定密语（passphrase，指允许包含标点符号的密码）为空；
    cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys：
        认证的主机（node1）登录该主机（也是node1）时，免用户密码（>>表示追加，>表示覆盖）；
        但（貌似）在“自己登录自己”时，不免ssh密语；
（5）修改hadoop中对JAVA的引用路径（否则ssh登录时有问题）：
    vim /opt/hadoop-2.6.5/etc/hadoop/hadoop-env.sh
        export JAVA_HOME=/usr/java/default
（6）修改浏览器访问（namenode服务器的）hadoop时，默认路径前缀的缩写（两个）：
    vim /opt/hadoop-2.6.5/etc/hadoop/core-site.xml
        <configuration>
        <!-- 指定访问HDFS的时候路径的默认前缀  /  hdfs://node1:9000/ -->
        <property>
        <name>fs.defaultFS</name>
        <value>hdfs://node1:9000</value>
        </property>
        <!-- 指定hadoop的临时目录位置，它会给namenode、secondarynamenode以及datanode的存储目录指定前缀 -->
        <property>
        <name>hadoop.tmp.dir</name>
        <value>/var/bjsxt/hadoop/pseudo</value>
        </property>
        </configuration>
    （可以不用手动创建/var/bjsxt/hadoop/pseudo目录，下面格式化hdfs时会自动创建）
（7）修改hdfs正常工作的参数：副本个数为1、secondarynamenode位置为http协议的node1:50090端口
    vim /opt/hadoop-2.6.5/etc/hadoop/hdfs-site.xml
        <configuration>
        <!-- 指定block副本数 -->
        <property>
        <name>dfs.replication</name>
        <value>1</value>
        </property>
        <!-- 指定secondarynamenode所在的位置 -->
        <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>node1:50090</value>
        </property>
        </configuration>
（8）修改datanode所在节点：（即该namenode会通过node1定位到datanode）
    vim /opt/hadoop-2.6.5/etc/hadoop/slaves：从文件名看出datanode是namenode的slave
        node1
（9）格式化（hdfs中的）namenode：
    hdfs namenode -format
    ls /var/bjsxt/hadoop/pseudo：检查该目录发现已成功创建，说明格式化命令会使用core-site.xml文件的配置
（10）启动hadoop：
    start-dfs.sh
        21/06/12 06:22:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable：
            正常警告，源码编译的才会没有
        Starting namenodes on [node1]
        node1: starting namenode, logging to /opt/hadoop-2.6.5/logs/hadoop-root-namenode-node1.out
        node1: starting datanode, logging to /opt/hadoop-2.6.5/logs/hadoop-root-datanode-node1.out
            这个是log，可供以后查看
        Starting secondary namenodes [node1]
        node1: starting secondarynamenode, logging to /opt/hadoop-2.6.5/logs/hadoop-root-secondarynamenode-node1.out
        21/06/12 06:22:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    jps：查看所有的java进程
        2448 DataNode
        2720 Jps
        2368 NameNode
        2618 SecondaryNameNode
2、浏览器体验搭建好的hadoop伪分布式环境：
（1）浏览器输入node1:50070：
    这个50070是默认的hadoop端口号，不是上面配置的端口
（2）首页overview下拉能看到datanode数量、空间使用的概览；
（3）点击live nodes可以看到datanode的详细信息：
    这里看到单个datanode=node1:50010，也是使用默认端口号，不是上面配置的端口
（4）点击utilities->browse the file system：
    输入/，点击go：看不到内容，因为这个hdfs系统创建好以后还没有保存过文件
（5）创建一个比较大的文本文件：
    cd /root
    for i in `seq 100000`; do echo "hello bjsxt $i" >> hello.txt; done
    cat hello.txt | head -n 5、cat hello.txt | tail -n 5：
        可以看到创建的文件包含100001行的字符串
（6）上传该文件到hdfs的根目录下：（简单操作）
    hdfs dfs -put /root/hello.txt /
    ls -lh /var/bjsxt/hadoop/pseudo/dfs/data/current/BP-1200552669-192.168.98.61-1623449821856/current/finalized/subdir0/subdir0/
        该命令可以看到文件上传成功后的实际占用大小为1.8M
    浏览器回到utilities->browse the file system，输入/，点击go：
        可以看到hello.txt，点开看到size=1.71M和默认blocksize=128M，只有1个副本为block0（对应hdfs-site.xml设置的默认副本数为1）
（7）再上传一个文件到hdfs的根目录下：（指定较小的blocksize，拆分block）
    cp hello.txt hh.txt
    hdfs dfs -D dfs.blocksize=1048576 -D dfs.replication=1 -put hh.txt /
        指定bocksize=1M，副本数=1
    cd /var/bjsxt/hadoop/pseudo/dfs/data/current/BP-1200552669-192.168.98.61-1623449821856/current/finalized/subdir0/subdir0/
    ls -lh
        该命令可以看到文件上传成功后拆分为两个文件，大小分别为1.0M、723k
    cat blk_1073741826 | tail -n 4
        hello bjsxt 58869
        hello bjsxt 58870
        hello bjsxt 58871
        hell
        可以看到文件的第一个block被截断
    cat blk_1073741827 | head -n 4
        o bjsxt 58872
        hello bjsxt 58873
        hello bjsxt 58874
        hello bjsxt 58875
        可以看到文件的第二个block接着被截断的地方开始
    浏览器回到utilities->browse the file system，输入/，点击go：
        可以看到hh.txt，点开看到size=1.71M和默认blocksize=1M，有1个副本是block1
（8）停止hdfs服务：
    stop-dfs.sh

0625：
1、P64：hadoop安全模式（了解）
（1）目的：（https://developer.aliyun.com/article/566059）
        Hadoop的安全模式在分布式文件系统启动的时候，开始的时候会有安全模式，当分布式文件系统处于安全模式的情况下，
        文件系统中的内容不允许修改也不允许删除，直到安全模式结束。 
        安全模式主要是为了系统启动的时候检查各个DataNode上数据块的有效性，同时根据策略必要的复制或者删除部分数据块。
（2）工作流程：
    2.1：启动namenode；
    2.2：namenode加载fsimage到内存，然后对内存数据执行edits.log（记录着上一次关闭namenode时还没来
        得及写入fsimage的记录）日志中的事务操作（即合并）；
        - 对于一般规模的集群，这个过程大约30min；
        - 这还是有secondarynamenode协助不断把edits.log合入fsimage、以减小edits.log内容的情况下的耗时；
    2.3：对fsimage+edits.log合并完成后，创建新的fsimage和一个空的edit.log；
        - 说明“fsimage+edits.log合并”操作不仅发生在集群正常工作期间的secondarynamenode上，还发生在处于集群启动
            阶段（即hadoop安全模式期间）的namenode上；
    2.4：然后namenode等待datanode上传block列表信息，直到副本数满足“最小副本条件”，namenode就会“倒计时”退出安全模式；
        - “最小副本条件”是指“整个文件系统中有99.9%的block达到了‘最小副本数’”；其中：
        - “最小副本数”的默认值是1，由hdfs-site.xml中的dfs.namenode.replication.min指定；
        - 退出安全模式的“倒计时”（？有什么用）由hdfs-site.xml中的dfs.namenode.safemode.extension值指定，单位ms；
        - “最小副本条件”对应的“99.9%”（我翻译为“安全模式阈值”）由hdfs-site.xml中的dfs.namenode.safemode.threshold-pct指定，
            比如0.999表示99.9%；
        - “namenode等待datanode上传block列表信息”所花的时间是值得的，因为这些block信息并没有在namenode进行持久化，
            需要上传到namenode才能被发现，而且越完整越好；
（3）命令操作示例：
    略；
2、P65-P66：（重要：面试题）hdfs写流程（文件上传）（《hadoop第1天.doc》文档里有框图）：
（1）基本概念：
    1.1：文件上传的操作涉及3个部分：
        - clientnode：跑着client JVM->hdfs client（从hdfs命令调用的/opt/haddop-2.6.5/bin/hdfs文件可以看到，
            实际是传参给java的class包去执行hdfs相关命令）；
        - namenode：为支持高并发，namonode自身不做数据传输、只做对clientnode的应答、和对datanode的管理；
        - datanode：有多个；
    1.2：实现机制的相关名词：
        - write packet：clientnode把block拆分成每个64k大小的packet，写到datanode（datanode之间的复制也是）；
            （默认的64k大小有core-default.xml的file.client-write-packet-size=65535指定）
        - pipeline（of datanode）：多个datanode之间的write packet复制、ack packet通知的机制；
        - ack packet：全部datanode完成对单个writepacket复制后对clientnode应答的packet（datanode之间的应答也是）；
        - dataqueue/ackqueue：clientnode发起一次写操作流程需要维护的两个队列；
            dataqueue中维护着拆分后的一系列64k writepacket
            ackqueue中维护着“正在等待writepacket复制到全部datanode、且未超时”的64k writepacket
                （如果超时，clientnode会检查管线上的哪个namenode掉线）；
（2）写文件（单个block）的正常流程：
    2.1：clientnode->namenode：clientnode向namenode请求“创建一个新文件”；
    2.2：namenode->clientnode：namenode检查文件是否已存在、clientnode是否有创建文件的权限、是否有足够的datanode可用；
        如果检查通过，则添加一个事务到edits.log，然后返回可用的datanode链给clientnode；
    2.3：clientnode->datanode1：clientnode创建dataqueue/ackqueue，拆分单个block为多个64k writepacket后填入；然后依次开始发送writepacket到第一个datanode；
    2.4：datanode1->datanode2->...->datanodeN：各datanode之间串行成管线，依序从1->N相互复制64k writepacket，然后从N->1相互应答ackpacket；
    2.5：datanode1->clientnode：datanode1每应答一次clientnode，表示各个datanode都收到了对应的单个64k writepacket；
    2.6：datanode->namenode：每个datanode接收最后一个64k writepacket后，表示该block数据已接收完整，就会各自通知namenode；
（3）写文件的异常流程：
    3.1：在clientnode->datanode1写文件过程中，管线上的某个datanode掉线：
        - 检测：clientnode对ackpacket的等到有超时判断:3s没收到就超时，会触发对管线的检查；
        - 处理：确定是管线上某个datanode掉线后，会把没收到ackpacket的对应writepacket都从ackqueue移动回dataqueue（writepacket一次只能存在一个队列中？），
                并把dataqueue中剩余的writepacket的blockid更新、通知namenode也更新blockid、更新datanode管线信息（剔除掉线的datanode）、使能重发；
        - 后处理：掉线的datanode“醒来”后，发现自己的blockid和namenode不同，就自动删掉自己之前收到的writepacket
                （而且因为此时已不属于新的管线，所以也不会再收到之前block的writepacket）；

0627：
1、P67：小测试：在clientnode上传大文件时手动中断（制造失败），会看到hdfs最终没有保存该文件
（1）启动hadoop：
    start-dfs.sh
    jps
（2）打开浏览器访问hdfs，准备等下上传大文件时不断刷新，查看进度：
    http://node1:50070->utilities->browse the file system->/,go
（3）上传一个171MB的文件jdk-8u221-linux-x64.rpm，并准备用ctrl+c中断上传过程：
    3.1：hdfs dfs -D dfs.blocksize=1048576 -D dfs.replication=1 -put jdk-8u221-linux-x64.rpm /
    3.2：在浏览器不断刷新，能看到有一个实时刷新size的文件：jdk-8u221-linux-x64.rpm._COPYING_；
    3.3：回到hdfs执行上传命令的窗口，ctrl+c中断；
    3.4：再在浏览器刷新，能看到jdk-8u221-linux-x64.rpm._COPYING_不见了；
    3.5：说明“如果上传过程中clientnode宕机，文件创建失败，之前上传的部分也会删掉”
（4）？实际参照视频中做实验时，出现报错，并hdfs没有自动删掉之前上传的部分：
    21/06/12 07:28:22 INFO fs.FileSystem: Ignoring failure to deleteOnExit for path hdfs://node1:9000/jdk-8u221-linux-x64.rpm._COPYING_
    put: java.nio.channels.ClosedChannelException
    at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:1618)
    at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:104)        ...
    ...
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
    at org.apache.hadoop.fs.FsShell.main(FsShell.java:340)
（5）备用命令：从hdfs删除文件：
    hadoop fs -rm -r -skipTrash /path_to_file/file_name
2、P68：（重要）hdfs读流程（按block读取，多个block组合成单个文件）
（1）clientnode中涉及读流程的结构：
    clientnode->
        HDFSclient（发起单个文件读取操作的客户端）->
            DistributedFileSystem（对象，发起单次对namenode读取）->namenode
            DFSInputStream（对象，发起按block对datanode读取）->datanode
（2）读文件的正常流程：
    2.1：clientnode的hdfsclient发起读流程：通过open方法打开希望读取的文件，然后按block让DistributedFileSystem
        （？通过RPC远程调用：效果是实时不断更新block的datanode信息？）
        从namenode请求文件开头第一批的block及其副本所在的datanode；
    2.2：DistributedFileSystem获取到目的datanode列表后，赋给DFSInputStream：
        此时DFSInputStream管理着datanode和namenode的IO，其中datanode信息来自对象DistributedFileSystem，namenode来自客户端HDFSclient；
    2.3：客户端HDFSclient开始调用read方法：（客户端只读取单个文件的连续数据流，不管理block读取的切换）
        - DFSInputStream从连接距离最近的datanode开始尝试读取block；
        - 读取完该block后，即关闭与该datanode的连接；
        - 然后寻找下一个block的最佳datanode；（重要：block读取的切换由DFSInputStream自动实现，以保证“读流程”对客户端的透明）
        - DFSInputStream对多个block的读取顺序不是按序号的，而是按照它与datanode新建连接的顺序（？所以datanode的负载会影响读到block的顺序？）
        - DFSInputStream读取第一批block后，就自动按照之前DistributedFileSystem给的namenode信息，再去获取第二批、甚至后续批次的block信息
            （block及其副本namenode的位置）
        - 客户端完成本次文件读取（可能是DFSInputStream读取完所有block后通知客户端，也可能客户端提前终止读取？）后，
            就close掉DFSInputStream的输入流；
（3）读文件的异常流程：
    3.1：DFSInputStream读block过程中发现某个block数据损坏（meta信息没对上），
        就会在本地记录该不损坏block所在的datanode，并立即上报给namenode（以全局更新信息），然后再尝试从其它datanode读取block副本；
        （？视频中说的是“客户端上报”，实际应该是“客户端下的DFSInputStream对象上报”？因为客户端HDFSclient不直接管理block）
    3.2：namenode得知某个block损坏后，就发起一次新的“复制事务操作”：从一个好的block副本复制该block到另一个datanode上；
        （此时只是block损坏，没有判断datanode掉线或宕机，所以namenode只处理单个block）
    3.3：（重要）DFSInputStream将不会“反复读取”该datanode上的后续block，即使namenode后续返回的列表里还有这个datanode：
        因为DFSInputStream发现有block损坏以后，这个datanode都不再是“最优节点”了；而实际上第一个“最优节点”是从距离远近来选取的，并不表示“性能最优”；
